

# 1 Pytorchä»‹ç»ğŸŒŸğŸŒŸ

## PART1 å¸¸è§çš„æ·±åº¦å­¦ä¹ æ¡†æ¶

å¦‚æœæ²¡æœ‰è¿™äº›æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæ·±åº¦å­¦ä¹ ç»å¯¹ä¸ä¼šåƒç°åœ¨ä¸€æ ·â€œå¹³æ°‘åŒ–â€ï¼Œå¾ˆå¤šäººå¯èƒ½é™·å…¥åœ¨èŒ«èŒ«çš„æ•°å­¦æ·±æ¸Šä¸­ã€‚æœ‰äº†å¯æ–¹ä¾¿ä½¿ç”¨çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠæ‰€æœ‰ç²¾åŠ›èŠ±åœ¨å¦‚ä½•è®¾è®¡æ¨¡å‹æœ¬èº«ä¸Šï¼Œè€Œä¸ç”¨å†å»å…³æ³¨æ¨¡å‹ä¼˜åŒ–çš„ç»†èŠ‚ï¼Œæ‰€æœ‰çš„äº‹æƒ…å‡ç”±æ¡†æ¶æ¥è´Ÿè´£ï¼Œæå¤§é™ä½äº†æ·±åº¦å­¦ä¹ ä½¿ç”¨çš„é—¨æ§›ã€‚è¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆç°åœ¨åªè¦ç»è¿‡çŸ­æœŸæœ‰æ•ˆè®­ç»ƒçš„å¼€å‘å·¥ç¨‹å¸ˆä¹Ÿå¯ä»¥åœ¨ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹èº«ä¸Šå¾—å¿ƒåº”æ‰‹çš„ä¸»è¦åŸå› ã€‚

æ·±åº¦å­¦ä¹ æ¡†æ¶çš„å‘å±•ä¹Ÿç»å†äº†è¶…è¿‡10å¹´çš„æ—¶é—´ï¼Œä»æ—©æœŸæ¯”è¾ƒæµè¡Œçš„theanoåˆ°ç°åœ¨æ¯”è¾ƒç«çˆ†çš„æ¡†æ¶å¦‚Pytorchï¼Œ Tensorflowï¼Œç»å†äº†å‡ ä¸ªé˜¶æ®µçš„å‘å±•å’Œè¿­ä»£ã€‚

![image-20210722221237044](/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210722221237044.png)

å›¾ä¸­å±•ç¤ºäº†å‡ ä¸ªæ¯”è¾ƒæœ‰ä»£è¡¨æ€§çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæ˜¯ä¸åŒæ—¶ä»£çš„äº§ç‰©ã€‚æ¯”å¦‚å›¾é‡Œçš„Caffeæ¥æºäºä¼¯å…‹åˆ©çš„ä¸€ä½åšå£«ç”Ÿï¼Œæ¡†æ¶æœ¬èº«æ•ˆç‡é«˜ï¼Œä½†éœ€è¦ç¼–å†™æ¯”è¾ƒç¹ççš„é…ç½®æ–‡ä»¶ã€‚åœ¨é…ç½®æ–‡ä»¶ä¸­ä¼šè®¾ç½®ç½‘ç»œçš„å±‚æ¬¡ã€æ¯ä¸€å±‚çš„å‚æ•°ç­‰æ‰€æœ‰ç»†èŠ‚ï¼Œç›®å‰åœ¨å·¥ä¸šç•Œä»ç„¶æ˜¯ä¸€ä¸ªæ¯”è¾ƒå—æ¬¢è¿çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚å¦å¤–ï¼ŒKerasçš„ä½¿ç”¨ä¹Ÿæ¯”è¾ƒå¹¿æ³›ã€‚å®ƒä¸€å¼€å§‹æ˜¯å»ºç«‹åœ¨Tensorflowä¹‹ä¸Šçš„ï¼Œå¹¶å°è£…äº†å¾ˆå¤šçš„æ¨¡å—ï¼Œè®©ä½¿ç”¨è€…å¯ä»¥æ›´ä½é—¨æ§›åœ°å»è®¾è®¡æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œç›®å‰ä¹Ÿæœ‰å¤§é‡çš„ä½¿ç”¨è€…ã€‚ä½†ç¼ºç‚¹æ˜¯ï¼Œç”±äºåšäº†è¿›ä¸€æ­¥çš„å°è£…ï¼Œå¦‚æœæƒ³åšä¸€äº›æ”¹åŠ¨ï¼Œçµæ´»æ€§ä¸Šç›¸æ¯”Tensorflowè¦å·®ä¸€äº›ã€‚

ä»è¿™äº›æ¡†æ¶ä¸­ï¼Œå¦‚æœè®©æˆ‘ä»¬é€‰æ‹©ç›®å‰æœ€ç«çˆ†çš„ï¼Œå¤§å¤šæ•°äººå¯èƒ½ä¼šæ¯«æ— çŠ¹è±«åœ°é€‰å‡ºPytorchå’ŒTensorFlowã€‚ç©¶å…¶åŸå› ï¼Œè¿˜æ˜¯å› ä¸ºä»–ä»¬çš„é«˜æ•ˆã€çµæ´»æ€§ä»¥åŠä½é—¨æ§›çš„ä½¿ç”¨ã€‚Tensorflowä½œä¸ºGoogleå…¬å¸ä¸€ä¸ªé‡è¦çš„äº§å“ï¼Œåœ¨æ€§èƒ½æ–¹é¢çš„è¡¨ç°ä¹Ÿæ˜¯å¯åœˆå¯ç‚¹çš„ã€‚å¦ä¸€æ–¹é¢ï¼ŒPytorchä½œä¸ºæ–°çš„æ¡†æ¶ï¼Œè¿™å‡ å¹´å±•ç°å‡ºäº†è¶…é«˜çš„äººæ°”å’Œå¢é•¿ï¼Œä¸»è¦æºäºå®ƒçš„ä½é—¨æ§›ä¸”ç‰¹åˆ«å®¹æ˜“ä¸Šæ‰‹ã€‚

## PART2 Pytorchæ¡†æ¶çš„å´›èµ·

åœ¨è¿™ä¸€èŠ‚ï¼Œæˆ‘ä»¬ä¸»è¦æ¯”è¾ƒTensorFlowä¸Pytorchä¸¤ä¸ªæ¡†æ¶çš„å‘å±•å†å²ä»¥åŠè¶‹åŠ¿ï¼Œåˆ†åˆ«ä»æœç´¢çƒ­åº¦ã€å­¦æœ¯ç•Œçš„æ¬¢è¿åº¦ç­‰è§’åº¦æ¥å‰–æã€‚ä¹‹æ‰€ä»¥é€‰æ‹©è¿™ä¸¤ä¸ªæ¡†æ¶ï¼Œä¸€æ–¹é¢çš„åŸå› åœ¨äºç¡®å®è¿™ä¿©æ˜¯ç›®å‰æœ€ç«çˆ†çš„æ¡†æ¶ï¼Œå¦å¤–ä¸€æ–¹é¢çš„åŸå› æ˜¯ä¹Ÿæ¯”è¾ƒé€‚åˆåˆšæ­¥å…¥AIé¢†åŸŸçš„äººå£«å»æ¥è§¦å’Œå­¦ä¹ ã€‚

![image-20210722221615091](/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210722221615091.png)

å›¾é‡Œå±•ç¤ºçš„æ˜¯Googleæœç´¢å¼•æ“ä¸Šçš„æœç´¢çƒ­åº¦ï¼Œä»£è¡¨æœ‰å¤šå°‘äººå»æœç´¢è¿™ä¸¤ä¸ªæ¡†æ¶ã€‚ä»å›¾ä¸­å¯ä»¥å¾ˆæ¸…æ¥šåœ°çœ‹åˆ°ï¼Œ17å¹´çš„æ—¶å€™TensorFlowä»ç„¶å æ®ç€å®Œå…¨ä¸»å¯¼æ€§çš„åœ°ä½ï¼Œä½†éšç€æ—¶é—´çš„æ¨ç§»ï¼ŒPytorchçš„å¢é•¿è¶Šæ¥è¶Šå¿«ï¼Œåˆ°äº†20å¹´åˆåŸºæœ¬ä¸Šé€¼è¿‘äº†Tensorflowçš„çƒ­åº¦ï¼Œè€Œä¸”è¿™ç§å¢é•¿è¶‹åŠ¿ä»åœ¨æŒç»­ã€‚

![image-20210722221733339](/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210722221733339.png)

![image-20210722221754439](/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210722221754439.png)

ä»¥ä¸Šä¸¤å¹…å›¾è¡¨ç¤ºçš„æ˜¯Pytorchå’ŒTensorFlowåœ¨å­¦æœ¯ç•Œçš„ä½¿ç”¨æƒ…å†µï¼Œåˆ†åˆ«ç®—å‡ºäº†æ¯ä¸€å¹´é¡¶ä¼šä¸­æœ‰å¤šå°‘ç¯‡æ–‡ç« çš„å®éªŒç”¨è¿™ä¸¤ä¸ªå·¥å…·æ¥åšçš„ã€‚æˆ‘ä»¬å¾ˆå®¹æ˜“å‘ç°ï¼Œåœ¨å­¦æœ¯ç•Œé‡ŒPytorchçš„ä¼˜åŠ¿æ›´åŠ æ˜æ˜¾ï¼Œæ˜¾ç¤ºå‡ºå¼ºåŠ¿çš„å¢é•¿ã€‚é‚£ä¸ºä»€ä¹ˆä¼šå‡ºç°è¿™ç§è¶‹åŠ¿å‘¢? ä¸»è¦è¿˜æ˜¯Pytorchç”¨èµ·æ¥ç®€å•ï¼Œè€Œä¸”æ•ˆç‡ä¹Ÿä¸å·®ã€‚å¯¹äºä¹‹å‰æ²¡æœ‰æ¥è§¦è¿‡æ·±åº¦å­¦ä¹ æ¡†æ¶çš„äººï¼ŒPytorchæ— ç–‘æ˜¯é¦–é€‰ï¼Œç‰¹åˆ«é€‚åˆå…¥é—¨ã€‚



## PART3 Pytorchä¸Tensorflowå¤šæ–¹ä½æ¯”è¾ƒ

![image-20210722221936819](/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210722221936819.png)

ä»¥ä¸Šå›¾ä¸­ç»™å‡ºäº†ä¸¤ä¸ªæ¡†æ¶ä¹‹é—´å…·ä½“çš„å·®å¼‚ï¼Œå…¶ä¸­æœ€é‡è¦çš„å·®åˆ«åœ¨äºPytorché‡‡ç”¨äº†å‘½ä»¤å¼ç¼–ç¨‹ï¼ŒTensorFlowåˆ™é‡‡ç”¨äº†ç¬¦å·å¼ç¼–ç¨‹(symbolic programming)ï¼Œå®é™…ä¸Šè¿™æ˜¯ä¸¤ç§å®Œå…¨ä¸åŒçš„ç¼–ç¨‹æ–¹å¼ã€‚å‘½ä»¤å¼ç¼–ç¨‹å…¶å®å°±æ˜¯æˆ‘ä»¬æœ€ç†Ÿæ‚‰çš„ç¼–ç¨‹æ–¹å¼ï¼Œæ¯”å¦‚ä½¿ç”¨Pythonï¼Œ Javaç­‰ç­‰ã€‚ç„¶è€Œï¼Œç¬¦å·å¼ç¼–ç¨‹å°±ä¸ä¸€æ ·äº†ï¼Œé¦–é€‰éœ€è¦æ„å»ºè®¡ç®—å›¾ï¼Œç„¶åå†æŠŠæ•°æ®çŒåˆ°å›¾é‡Œåšè®¡ç®—ã€‚

ä¸ºäº†ç†è§£ä¸Šè¿°è§‚ç‚¹ï¼Œç®€å•çœ‹ä¸€ä¸‹ç»™å‡ºçš„å‡ è¡Œä»£ç ã€‚ å·¦è¾¹å±•ç¤ºçš„æ˜¯Pytorchæ¡†æ¶ä¸‹çš„ç¨‹åºï¼Œè·Ÿæˆ‘ä»¬æ—¥å¸¸ç¼–å†™çš„ç¨‹åºæ²¡ä»€ä¹ˆå·®å¼‚ã€‚ä¸ºäº†è®¡ç®—`aÃ—b+1`ï¼Œ æˆ‘ä»¬é€ä¸ªå»å®šä¹‰ï¼Œå¹¶ä¸æ–­åœ°é€šè¿‡æ¼”ç®—æœ€ç»ˆå¾—å‡ºç»“æœã€‚

å¦‚æœæ”¾åœ¨Tensorflowå°±ä¸ä¸€æ ·äº†ï¼Œæˆ‘ä»¬é¦–å…ˆæ„é€ äº†ä¸€ä¸ªé™æ€çš„è®¡ç®—å›¾(computation graph)ï¼Œç„¶åæŠŠå˜é‡ä¹‹é—´çš„å…³ç³»å…ˆç¡®å®šå¥½ã€‚åœ¨è¿™é‡Œï¼Œå˜é‡DDä¸ºæœ€åçš„è¾“å‡ºèŠ‚ç‚¹ã€‚å®šä¹‰å¥½é™æ€è®¡ç®—å›¾ä¹‹åï¼Œæˆ‘ä»¬å°±å¯ä»¥æŠŠæ•°æ®è¾“å…¥ç»™è®¡ç®—å›¾äº†ã€‚è¾“å…¥æ•°æ®æ¥ç€ä¼šé€šè¿‡é¢„å…ˆå®šä¹‰å¥½çš„æ­¥éª¤æœ€åèƒ½ç®—å‡ºç»“æœã€‚

å¦‚æœå¯¹ä¸Šè¿°æ¦‚å¿µæ¯”è¾ƒéš¾ç†è§£ï¼Œä½ ä¹Ÿå¯ä»¥æƒ³è±¡ä¸€ä¸ªè¿™æ ·çš„åœºæ™¯ã€‚æœ‰ä¸€å®¶å…¬å¸ç°åœ¨è¯•ç€å»æ„å»ºä»åŸå¸‚Aåˆ°Bçš„ç®¡é“ï¼Œç”¨æ¥è¿è¾“ä¸€å®šé‡çš„çŸ³æ²¹ã€‚ä¸€ç§è§£å†³æ€è·¯æ˜¯ï¼Œæå‰æŠŠç®¡é“å…¨éƒ¨åˆ¶ä½œå®Œæˆï¼Œç„¶åæŠŠçŸ³æ²¹è¾“å…¥åˆ°ç®¡é“ä¸­ï¼Œä¹‹åé€šè¿‡ä¸€ç³»åˆ—è¿è¾“è¿‡ç¨‹æœ€ç»ˆå¯èƒ½ä¼šåˆ°è¾¾BåŸå¸‚ã€‚å¦å¤–ä¸€ç§è§£å†³æ€è·¯æ˜¯ï¼Œæˆ‘ä»¬ä¸€è¾¹åˆ¶ä½œç®¡é“ï¼Œä¸€è¾¹è¿è¾“çŸ³æ²¹ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ç®¡é“çš„è®¾è®¡å¯ä»¥åŠ¨æ€åœ°æ”¹å˜ï¼Œæ¯”å¦‚æˆ‘ä»¬å‘ç°æŸä¸ªè·¯å¾„ä¸å¯¹åŠ²ï¼Œå°±å¯ä»¥æ¢æˆå¦å¤–ä¸€ä¸ªè·¯å¾„ã€‚ åœ¨è¿™é‡Œä¾‹å­ä¸­ï¼Œå‰è€…å¯¹åº”çš„æ˜¯ç¬¦å·å¼ç¼–ç¨‹ï¼Œåè€…å¯¹åº”çš„æ˜¯å‘½ä»¤å¼ç¼–ç¨‹ã€‚ ç®€ç­”æ¥è®²ï¼Œå‰è€…æ˜¯é™æ€çš„ï¼Œåè€…æ˜¯åŠ¨æ€çš„ã€‚åŠ¨æ€çš„å¥½å¤„æ˜¯çµæ´»ï¼Œä½†ç¼ºç‚¹æ˜¯æ•ˆç‡ä¼šä½ä¸€äº›;ç›¸åï¼Œå‰è€…æ˜¯é™æ€çš„ï¼Œå¿…é¡»è¦æå‰å‡†å¤‡å¥½å®Œæ•´çš„è®¡ç®—å›¾(ç®¡é“)ï¼Œä¹‹åæ‰èƒ½ä½¿ç”¨ï¼Œè¿™ç§ä¼˜åŠ¿åœ¨äºä½¿ç”¨æ—¶çš„æ•ˆç‡é«˜ï¼Œä½†ç¼ºç‚¹æ˜¯ä¸å¥½ç†è§£å’Œdebugã€‚

åœ¨è¿™ä¸ªè¯¾ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†é‡‡ç”¨Pytorchæ¡†æ¶ï¼Œä¹Ÿå»ºè®®åˆšæ­¥å…¥AIé¢†åŸŸçš„äººå£«ä½¿ç”¨Pytorchï¼Œä¼šå¤§å¤§é™ä½å­¦ä¹ æˆæœ¬ã€‚åœ¨ä¹‹åçš„ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä¸»è¦æ¥ä»‹ç»Pytorchçš„å¸¸è§çš„ä½¿ç”¨æ–¹æ³•ã€‚

# 2 TensorsğŸŒŸğŸŒŸğŸŒŸ

å­¦ä¹ ç›®æ ‡ï¼š*åœ¨æœ¬èŠ‚æˆ‘ä»¬ä¸»è¦ä»‹ç»Tensorçš„æ¦‚å¿µä»¥åŠç›¸å…³çš„è¿ç®—æ“ä½œã€‚è¿™ä¸€èŠ‚çš„å¾ˆå¤šå†…å®¹æ¥æºäºPytorchå®˜ç½‘ã€‚*

ç›¸å…³çŸ¥è¯†ç‚¹ï¼š

*Tensor*

## PART1 Tensorçš„åˆ›å»º

æˆ‘ä»¬é¦–å…ˆéœ€è¦ç†è§£Tensorè¿™ä¸ªå…³é”®è¯ï¼Œè¿™æ˜¯Pytorchä¸­æœ€åŸºç¡€çš„æ•°æ®ç»“æ„ï¼Œç±»ä¼¼äºNumpyåº“ä¸­çš„arrayï¼Œ matrixä¸€æ ·ã€‚ä½†åœ¨Pytorchæˆ‘ä»¬æŠŠè¿™äº›ç»Ÿä¸€å®šä¹‰ä¸ºTensorã€‚ä¸ºä»€ä¹ˆè¦èµ·è¿™ä¸ªåå­—å‘¢? è¿™ä¸€ç‚¹å…¶å®æˆ‘ä»¬åœ¨å‰é¢çš„è¯¾ç¨‹ä¸­æœ‰è®²è¿‡ã€‚æ•°æ®çš„è¡¨ç°å½¢å¼é€šå¸¸ä¸ºæ ‡é‡(scalar)ã€å‘é‡(vector)ã€çŸ©é˜µ(matrix)ã€å¼ é‡(Tensor)ã€‚ å…¶ä¸­æ ‡é‡å¯ä»¥çœ‹ä½œæ˜¯0ç»´çš„å¼ é‡ã€å‘é‡çœ‹ä½œæ˜¯1ç»´çš„å¼ é‡ã€çŸ©é˜µçœ‹ä½œæ˜¯2ç»´çš„å¼ é‡ï¼Œä¾æ¬¡ç±»æ¨ã€‚æ‰€ä»¥ï¼Œæœ€ç»ˆæˆ‘ä»¬å¯ä»¥æŠŠTensorä½œä¸ºè¿™äº›æ•°æ®ç»“æ„çš„ç»Ÿç§°ï¼Œè¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆåƒTensorFlowè¿™ç§æ¡†æ¶é‡ŒåŒ…å«Tensorå…³é”®è¯çš„ä¸»è¦åŸå› ã€‚

**åœ¨Pytorchä¸­ï¼ŒTensorçš„ä½¿ç”¨éå¸¸ç±»ä¼¼äºNumpyçš„ç”¨æ³•ï¼Œä½†åŒºåˆ«äºNumpyçš„æ•°æ®ï¼ŒTensoræ•°æ®å¯ä»¥ç”¨åœ¨GPUç­‰è®¾å¤‡ä¸Šå»è·‘ï¼Œå¯ä»¥å¤§å¤§æé«˜ç®—æ³•è¿è¡Œçš„æ•ˆç‡ã€‚**

### Tensoråº“çš„å¯¼å…¥

ä¸ºäº†ä½¿ç”¨Pytorchçš„æ•°æ®ç»“æ„ä¸åŠŸèƒ½ï¼Œé¦–å…ˆéœ€è¦å¯¼å…¥ç›¸åº”çš„åº“ã€‚è¿™ç±»ä¼¼äºå½“ä½¿ç”¨Numpyçš„æ—¶å€™å¯¼å…¥numpyåº“ä¸€æ ·ã€‚å¯¹äºPytorchï¼Œæˆ‘ä»¬å¯ä»¥å¯¼å…¥torchåº“ã€‚

```PYTHON
import torch

import numpy as np
```

### ä»å·²æœ‰æ•°æ®ç›´æ¥æ„å»ºTensor

ç¬¬ä¸€æ­¥æ˜¯æ„å»ºTensorç±»å‹çš„æ•°æ®ï¼Œå…¶ä¸­ä¸€ä¸ªæ–¹æ³•æ˜¯ç›´æ¥åˆ©ç”¨å·²æœ‰çš„æ•°æ®æ¥åˆå§‹åŒ–Tensorï¼Œå¦‚ä¸‹æ‰€ç¤º:

```python
data = [[1,3],[3,4]]

t_data = torch.tensor(data)
```

### æŠŠNumpyæ•°æ®è½¬æ¢æˆTensorç±»å‹

å¦‚æœæ•°æ®å·²ç»è¡¨ç¤ºä¸ºNumpyç±»å‹ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ç›´æ¥æŠŠå®ƒè½¬æ¢ä¸ºTensorç±»å‹çš„æ•°æ®ï¼Œè¿™ç§æ“ä½œåœ¨å®é™…é¡¹ç›®ä¸­éå¸¸å®ç”¨ã€‚

```python
np_data = np.array(data)

t_data = torch.from_numpy(np_array)
#æˆ–
t_data = torch.tensor(np_array)
```

### ç›´æ¥åˆ©ç”¨Tensoråº“æ¥åˆ›å»ºTensoræ•°æ®

å¦å¤–ä¸€ç§æ–¹å¼æ˜¯ç›´æ¥ä½¿ç”¨Tensoræ‰€æä¾›çš„æ–¹æ³•æ¥æ„é€ Tensoræ•°æ®ï¼Œè¿™ç±»ä¼¼äºæˆ‘ä»¬è°ƒç”¨numpyã€‚zeors()å‡½æ•°æ¥åˆ›å»ºnumpyå‹æ•°æ®ä¸€æ ·ã€‚è¯·çœ‹å¦‚ä¸‹å‡ è¡Œä»£ç :

```python
shape = (2,3,)

rand_tensor = torch.rand(shape)

ones_tensor = torch.ones(shape)

zeros_tensor = torch.zeros(shape)
```

### Tensorçš„å±æ€§(attributes)

æ„å»ºå¥½Tensorä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥æŸ¥çœ‹å®ƒçš„ä¸€äº›å±æ€§å¦‚å¤§å°ã€ç±»å‹ã€ä»¥åŠå­˜æ”¾åœ¨cpuè¿˜æ˜¯åœ¨gpuç­‰ä¿¡æ¯ã€‚ä»¥ä¸Šå±æ€§ä¾æ¬¡é€šè¿‡shapeï¼Œ dtypeï¼Œ deviceå…³é”®è¯æ¥è·å–ã€‚ è¯·è¿è¡Œä¸‹æ–¹çš„ä»£ç å¹¶æŸ¥çœ‹è¾“å‡ºç»“æœã€‚

```python
import torch
import numpy as np
data = torch.rand(3,4)
print(f"Shape of data: {data.shape}")
print(f"Datatype of data: {data.dtype}")
print(f"Device data is stored on: {data.device}")

```



## PART2 Tensorçš„æ“ä½œ

Tensorä¹ŸåƒNumpy arrayæ”¯æŒå„ç§å„æ ·çš„è¿ç®—æ“ä½œï¼Œæ¯”å¦‚çŸ©é˜µä¹˜æ³•ã€åŠ æ³•ã€é‡‡æ ·ç­‰ç­‰ï¼Œè€Œä¸”è¿™äº›è¿ç®—å‡å¯ä»¥åœ¨GPUä¸Šè¿›è¡Œã€‚å¦‚æœæƒ³æŠŠ Tensoråœ¨GPUåšè®¡ç®—ï¼Œéœ€è¦æŠŠå®ƒå…ˆæŒªåˆ°GPUå†…å­˜ä¸­ï¼Œé€šè¿‡ä»¥ä¸‹å‡ è¡Œä»£ç å°±å¯ä»¥å®ç°:

```python
if torch.cuda.is_available():

tensor = tensor.to('cuda')
```

### Tensorçš„ç´¢å¼•

å¯¹äºTensorï¼Œ æˆ‘ä»¬å¯ä»¥å¾ˆæ–¹ä¾¿çš„æå–å®ƒçš„æŸä¸€è¡Œã€æŸä¸€åˆ—ã€æˆ–è€…å¤šè¡Œã€å¤šåˆ—ï¼Œä½¿ç”¨æ–¹æ³•è·Ÿnumpyå‡ ä¹ä¸€æ¨¡ä¸€æ ·ã€‚

```python
data = torch.ones(4,4)

data[:,1] = 0
```

### å¤šä¸ªTensorçš„æ‹¼æ¥

å¾ˆå¤šæ—¶å€™ï¼Œæˆ‘ä»¬éœ€è¦æŠŠå¤šä¸ªTensoråšæ‹¼æ¥ï¼Œå¹¶è½¬æ¢ä¸ºæ›´å¤§çš„Tensorã€‚ è¿™ç§æ“ä½œå¯ä»¥é€šè¿‡è‡ªå¸¦çš„torchã€‚cat()æ¥å®Œæˆï¼Œå…·ä½“ä»¥å“ªä¸ªæ–¹å‘åšæ‹¼æ¥ç”±dimå‚æ•°æ¥è®¾å®šã€‚

```python
t1 = torch.cat([data, data, data], dim =1)
```

### Tensorçš„ä¹˜æ³•

ç»™å®šä¸¤ä¸ªTensorä¹Ÿå¯ä»¥æ–¹ä¾¿åœ°å®Œæˆä¹˜æ³•è¿ç®—ã€‚è¿™é‡Œéœ€è¦æ³¨æ„çš„ä¸€ç‚¹æ˜¯ï¼Œä¸€ç§ä¹˜æ³•è¿ç®—å¯ä»¥æ˜¯æˆ‘ä»¬æ‰€ç†ŸçŸ¥çš„æ­£å¸¸çš„çŸ©é˜µä¹˜æ³•è¿ç®—ï¼Œå¦å¤–ä¸€ç§ä¹˜æ³•è¿ç®—æ˜¯æŒ‰ç…§æ¯ä¸€ä¸ªä½ç½®çš„**ä¹˜æ³•è¿ç®—(element-wise multiplication)**

```python
data1 = torch.ones(2,2)

data2 = torch.ones(2,2)

mul_res1 = torch.matmul(data1, data2)

mul_res2 = data1 * data2
```

```python

import torch
import numpy as np
# index tensor array
data = torch.ones(4,4)
data[:,1] = 0
print(f"Slicing example: \n{data} \n")
# concatenate 3 tensors
data = torch.rand(3,3)
t1 = torch.cat([data, data, data], dim =1)
print(f"Concatenation of tensor example before: \n {data} \n")
print(f"Concatenation of tensor example after: \n {t1} \n")
# multiply tensors
data1 = torch.ones(2,2)
data2 = torch.ones(2,2)
mul_res1 = torch.matmul(data1, data2)  # normal multiplication
mul_res2 = data1 * data2  # element-wise multiplication
print(f"normal multiplication example: data1: \n {mul_res1} \n")
print(f"normal multiplication example: data2: \n {mul_res1} \n")
print(f"normal multiplication example: mul_res1: \n {mul_res1} \n")
print(f"element-wise multiplication example: mul_res2 \n {mul_res2} \n")
```

[æ›´å¤šç”¨æ³•è§æ­¤åšå®¢](https://www.cnblogs.com/piaodoo/p/13936333.html)



## PART3 Tensorä¸Numpyä¹‹é—´çš„è½¬æ¢

### ä»Tensoråˆ°Numpy

åœ¨CPUä¸Šï¼ŒTensorå’ŒNumpyå˜é‡å¯ä»¥å…±äº«ä¸€ä¸ªå†…å­˜ç©ºé—´ï¼Œæ”¹å˜å…¶ä¸­ä¸€ä¸ªä¼šè‡ªåŠ¨æ”¹å˜å¦å¤–ä¸€ä¸ªã€‚ä»Tensoråˆ°numpyç±»å‹çš„è½¬åŒ–é€šè¿‡å‡½æ•°numpy()å³å¯ä»¥å®ç°ã€‚

```python
t = torch.ones(5)

n = t.numpy()
```

### ä»Numpyåˆ°Tensorçš„è½¬æ¢

å¦ä¸€ä¸ªæ–¹å‘çš„è½¬æ¢ä¹Ÿæå…¶ç®€å•ï¼Œå¯é€šè¿‡from_numpy()å‡½æ•°æ¥å®Œæˆã€‚è¿™ç§æƒ…å†µä¸‹ä¸¤ä¸ªå˜é‡ä¼šå…±äº«ä¸€ä¸ªå†…å­˜ï¼Œæ”¹å˜å…¶ä¸­ä¸€ä¸ªä¹Ÿä¼šæ”¹å˜å¦å¤–ä¸€ä¸ªå˜é‡ï¼Œè¿™ä¸€ç‚¹éœ€è¦ç•™æ„ä¸€ä¸‹ã€‚

```python
n = np.ones(5)

t = torch.from_numpy(n)
```



```python
import torch
import numpy as np
# from torch to numpy
t = torch.ones(5)
print(f"t: {t}")
n = t.numpy()
# change one will change another
t.add_(1)
print(f"t: {t}")
print(f"n: {n}")
# from numpy to torch
n = np.ones(5)
t = torch.from_numpy(n)
# change one will change another
np.add(n, 1, out=n)
print(f"t: {t}")
print(f"n: {n}")
```



# 3 Autogradçš„è®²è§£ğŸŒŸğŸŒŸğŸŒŸ

å­¦ä¹ ç›®æ ‡ï¼š*æœ¬èŠ‚ä¸»è¦ä»‹ç»Pytorchä¸­Autogradæ¨¡å—çš„ä½œç”¨ä»¥åŠç”¨æ³•ã€‚*

ç›¸å…³çŸ¥è¯†ç‚¹ï¼š

*å‰å‘ä¼ æ’­ï¼Œåå‘ä¼ æ’­*

## PART1 æ¨¡å‹ä¸­çš„å‰å‘ä¼ æ’­ä¸åå‘ä¼ æ’­

åœ¨ä¸Šä¸€ç« é‡Œï¼Œæˆ‘ä»¬å·²ç»ä»‹ç»è¿‡ç¥ç»ç½‘ç»œä¸­çš„å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­çš„æ¦‚å¿µï¼Œåœ¨è¿™åšä¸€ä¸ªç®€å•çš„å›é¡¾ã€‚å¯¹äºç¥ç»ç½‘ç»œçš„ä¼˜åŒ–ï¼Œä¸€èˆ¬åˆ†ä¸ºä¸¤ä¸ªæ­¥éª¤:ç¬¬ä¸€æ­¥ä¸ºå‰å‘ä¼ æ’­ï¼Œä¹Ÿå°±æ˜¯ç»™å®šè®­ç»ƒæ•°æ®ï¼Œé€šè¿‡å‰å‘ä¼ æ’­è®¡ç®—å‡ºæ¨¡å‹ä¸­æ¯ä¸ªèŠ‚ç‚¹çš„è¾“å‡º;ç¬¬äºŒæ­¥åˆ™ä¸ºåå‘ä¼ æ’­ï¼Œé€šè¿‡è¿™ä¸€æ­¥è®¡ç®—å‡ºæ¯ä¸€ä¸ªå‚æ•°çš„æ¢¯åº¦ï¼Œæœ€ååšå‚æ•°çš„æ›´æ–°ã€‚å®é™…ä¸Šï¼ŒPytorchä¸­çš„autogradæ¨¡å—å°±æ˜¯æ›¿æˆ‘ä»¬å®Œæˆè¿™äº›äº‹æƒ…! 

***ï¼ˆåå‘ä¼ æ’­æ˜¯ä¸ºäº†è®¡ç®—æ¢¯åº¦ï¼‰***

ä¸‹é¢ï¼Œæˆ‘ä»¬æ¥çœ‹ä¸€ä¸ªå…·ä½“çš„ä¾‹å­ã€‚é¦–å…ˆï¼Œå¯¼å…¥å·²ç»è®­ç»ƒå¥½çš„restnetæ¨¡å‹ï¼ŒåŒæ—¶ä¹Ÿæ„å»ºä¸€ä¸ªéšæœºæ ·æœ¬ã€‚è¿™ä¸ªæ ·æœ¬ä¸ºä¸€å¼ 64*64çš„å›¾ç‰‡ä¸”æ¯ä¸€ä¸ªåƒç´ ç”±RGBæ¥è¡¨ç¤ºï¼Œå¯¹åº”çš„æ ‡ç­¾ä¸ºä¸€ä¸ªæ•´æ•°ã€‚

```python
import torch, torchvision
model = torchvision.models.resnet18(pretrained=True)
data = torch.rand(1, 3, 64, 64)
labels = torch.rand(1, 1000)
for itr in range(10):
  prediction = model(data) # forward pass
  loss = torch.abs(prediction - labels).sum()
  loss.backward() # backward pass
  optim = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9)
  optim.step() #gradient descent
 
  print (loss)
```

## PART2 åˆ©ç”¨autogradè®¡ç®—æ¢¯åº¦

å¯¹äºautogradå†çœ‹ä¸€ä¸ªä¾‹å­ï¼Œç”¨æ¥åŠ æ·±å¯¹å®ƒçš„ç†è§£ã€‚å‡å¦‚æœ‰ä¸¤ä¸ªTensoråˆ†åˆ«ä¸ºaå’Œbï¼Œ åŒæ—¶**è®¾ç½®requires_grad=True**ï¼Œ è¿™æ ·çš„ç»“æœå°±æ˜¯autogradä¼šä¿å­˜å¯¹äºç›¸åº”å˜é‡çš„æ“ä½œã€‚

![image-20210723215454165](/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210723215454165.png)





# 4 æ„å»ºç¥ç»ç½‘ç»œæ¨¡å‹ğŸŒŸğŸŒŸğŸŒŸ

å­¦ä¹ ç›®æ ‡ï¼š*æœ¬èŠ‚ä¸»è¦è®²è§£å¦‚ä½•ä½¿ç”¨Pytorchä»é›¶æ­å»ºä¸€ä¸ªç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œå¹¶åšè®­ç»ƒã€‚*

ç›¸å…³çŸ¥è¯†ç‚¹ï¼š

*Pytorchçš„ä½¿ç”¨*

æ­å»ºçš„è¿‡ç¨‹ä¸»è¦åˆ†ä¸ºä»¥ä¸‹å‡ æ­¥:

1ã€‚ æ•°æ®çš„æ„é€ ï¼Œè¿™éƒ¨åˆ†ä¸€èˆ¬éœ€è¦é€šè¿‡ä¸€äº›å¤„ç†ï¼Œè·Ÿä¹‹å‰çš„åšæ³•æ²¡ä»€ä¹ˆåŒºåˆ«ã€‚å¦‚æœæœ‰åŒºåˆ«ï¼Œå°±æ˜¯éœ€è¦æŠŠæ•°æ®åšæˆTensorç±»å‹ã€‚

2ã€‚ æ¨¡å‹çš„æ„é€ ï¼Œè¿™æ˜¯æ ¸å¿ƒï¼Œä¹Ÿæ˜¯Pytorchæä¾›ç»™æˆ‘ä»¬çš„ä¾¿æ·çš„åœ°æ–¹ã€‚

3ã€‚ ä¼˜åŒ–ç›¸å…³çš„è®¾ç½®ï¼Œè¿™ä¸€å—ä¸»è¦è®¾ç½®optimizerçš„é€‰æ‹©ä»¥åŠé…ç½®ç­‰ä¿¡æ¯ã€‚ 

4ã€‚ è®­ç»ƒæ¨¡å‹ï¼Œè¿™ä¸€éƒ¨åˆ†éœ€è¦å¾ªç¯æˆ‘ä»¬çš„è®­ç»ƒæ•°æ®ï¼Œå¹¶ä¸€æ­¥æ­¥é€šè¿‡optimizeræ¥ä¼˜åŒ–æ¨¡å‹çš„å‚æ•°ã€‚

## PART1 æ•°æ®çš„æ„é€ 

### æ•°æ®çš„æ„é€ 

è‡³äºæ•°æ®è¿™å—ï¼Œä¸ºäº†ç®€å•æœŸé—´ï¼Œå…ˆç”¨ä¸€ä¸ªæ¨¡æ‹Ÿçš„æ•°æ®æ¥ä»£æ›¿ï¼Œè€Œä¸”è¿™å¹¶ä¸å½±å“æˆ‘ä»¬å¯¹åç»­ç¯èŠ‚çš„ç†è§£ã€‚

```python
# make fake data
n_data = torch.ones(100, 2)
x0 = torch.normal(2*n_data, 1)      # class0 x data (tensor), shape=(100, 2)
y0 = torch.zeros(100)               # class0 y data (tensor), shape=(100, 1)
x1 = torch.normal(-2*n_data, 1)     # class1 x data (tensor), shape=(100, 2)
y1 = torch.ones(100)                # class1 y data (tensor), shape=(100, 1)
x = torch.cat((x0, x1), 0).type(torch.FloatTensor)  # shape (200, 2) FloatTensor = 32-bit floating
y = torch.cat((y0, y1), ).type(torch.LongTensor)    # shape (200,) LongTensor = 64-bit integer
```

## PART2 æ¨¡å‹çš„æ„é€ 

### æ¨¡å‹çš„æ„é€ 

å¯¹äºæ¨¡å‹è¿™éƒ¨åˆ†ï¼Œæˆ‘ä»¬éœ€è¦è®¾è®¡çš„æ˜¯å‰å‘ä¼ æ’­éƒ¨åˆ†(forward)ï¼Œå› ä¸ºè¿™éƒ¨åˆ†å…¶å®å†³å®šäº†æ•´ä¸ªæ¨¡å‹çš„ç»†èŠ‚ï¼Œæ¯”å¦‚ä¸€ä¸ªæ•°æ®xè¿›å…¥æ¨¡å‹ä¹‹åï¼Œå¦‚ä½•ä¸€æ­¥æ­¥è½¬æ¢æˆæœ€ç»ˆçš„è¾“å‡ºã€‚è½¬æ¢ç»†èŠ‚å®é™…ä¸Šå°±æ˜¯æ¨¡å‹çš„ç»†èŠ‚ã€‚ åœ¨æ„å»ºæ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šåˆ›å»ºä¸€ä¸ªæ–°çš„ç±»(class)ï¼Œå¹¶èµ·ä¸€ä¸ªåˆé€‚çš„åå­—ç»™åˆ°ç¥ç»ç½‘ç»œï¼Œä¹‹ååœ¨åˆå§‹åŒ–é˜¶æ®µå®šä¹‰æ¨¡å‹ä¸­æ‰€ä½¿ç”¨çš„å‚æ•°å’Œéƒ¨ä»¶ï¼Œæ¥ç€åœ¨forward()å‡½æ•°ä¸­è®¾è®¡è¾“å…¥åˆ°è¾“å‡ºä¸­æ‰€ç»å†çš„æ‰€æœ‰çš„è¿‡ç¨‹ã€‚

```python
class Net(torch.nn.Module):
  
  def __init__(self, n_feature, n_hidden, n_output):
    super(Net, self).__init__()
    self.hidden = torch.nn.Linear(n_feature, n_hidden)   # hidden layer
    self.out = torch.nn.Linear(n_hidden, n_output)   # output layer

  def forward(self, x):
    x = F.relu(self.hidden(x))      # activation function for hidden layer
    x = self.out(x)
    return x

net = Net(n_feature=2, n_hidden=10, n_output=2)     # define the network
```

## PART3 ä¼˜åŒ–å™¨é€‰æ‹©å’Œé…ç½®

è®¾è®¡å¥½äº†æ¨¡å‹ä¹‹åï¼Œå‰©ä¸‹çš„å·¥ä½œå°±æ˜¯è®¾è®¡losså’Œé…ç½®ä¼˜åŒ–å™¨ã€‚åœ¨æ¨¡å‹ä¸­æˆ‘ä»¬å®šä¹‰äº†`forward()`å‡½æ•°å†…å®¹ï¼Œé€šè¿‡è¿™ä¸ªå‡½æ•°å°±å¯ä»¥å¾—åˆ°å¯¹äºè¾“å…¥çš„é¢„æµ‹ã€‚æœ‰äº†é¢„æµ‹å°±å¯ä»¥è·ŸçœŸå®å€¼åšæ¯”è¾ƒæ¥è®¡ç®—æŸå¤±äº†ã€‚æ‰€ä»¥é¦–å…ˆè¦å®šä¹‰æŸå¤±å‡½æ•°çš„å½¢æ€ï¼Œæ˜¯ä½¿ç”¨**MSE**è¿˜æ˜¯äº¤å‰ç†µæŸå¤±ï¼Œè¿˜æ˜¯**Hinge Loss**? å½“ç„¶ï¼Œè¿™äº›å–å†³äºé—®é¢˜æœ¬èº«ã€‚åœ¨ä¸Šè¿°ä¾‹å­ä¸­ï¼Œç”±äºé—®é¢˜æ˜¯äºŒåˆ†ç±»é—®é¢˜ï¼Œæˆ‘ä»¬å†³å®šé€‰æ‹©**äº¤å‰ç†µæŸå¤±(entropy loss)**ã€‚

```python
loss_func = torch.nn.CrossEntropyLoss()  # the target label is NOT an one-hotted
```



## PART4 æœ€åä¸€æ­¥ï¼Œä¸»å‡½æ•°éƒ¨åˆ†

å®Œæˆäº†æ‰€æœ‰ä¸Šè¿°æ­¥éª¤ä¹‹åï¼Œå‰©ä¸‹çš„å°±æ˜¯ä¸»å‡½æ•°éƒ¨åˆ†äº†ã€‚åœ¨è¿™é‡Œéœ€è¦å®šä¹‰è¦å¾ªç¯å¤šå°‘æ¬¡(epoch)ï¼Œå¦‚ä½•ä¿å­˜ä¸­é—´ç»“æœï¼Œå¦‚ä½•è¾“å‡ºå‡†ç¡®ç‡ç­‰å†…å®¹ã€‚

```python
for t in range(50):
  out = net(x)
  loss = loss_func(out, y)
  optimizer.zero_grad()   # clear gradients for next train
  loss.backward()         # backpropagation, compute gradients
  optimizer.step()        # apply gradients

  if t % 2 == 0:
    prediction = torch.max(out, 1)[1]
    pred_y = prediction.data.numpy()
    target_y = y.data.numpy()
    accuracy = float((pred_y == target_y).astype(int).sum()) / float(target_y.size)
    print ('Accuracy=%.2f' % accuracy)
    plt.pause(0.1)
```

æ€»ç»“ï¼Œå››æ­¥ï¼š

- å‰å‘ä¼ æ’­`out = net(x)`
- è®¡ç®—loss`loss = loss_func(out, y)`  ï¼ˆæ¸…æ¥šgradientæ˜¯è¾…åŠ©æ­¥éª¤ï¼‰
- åå‘ä¼ æ’­ï¼ˆè®¡ç®—æ¢¯åº¦ï¼‰`loss.backward()  `
- åº”ç”¨æ¢¯åº¦` optimizer.step()  `

## PART5 å®Œæ•´çš„ç¨‹åº

```python
"""
View more, visit my tutorial page: https://mofanpy.com/tutorials/
My Youtube Channel: https://www.youtube.com/user/MorvanZhou
Dependencies:
torch: 0.4
matplotlib
"""
import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt
# torch.manual_seed(1)  # reproducible
# make fake data
n_data = torch.ones(100, 2)
x0 = torch.normal(2*n_data, 1)   # class0 x data (tensor), shape=(100, 2)
y0 = torch.zeros(100)       # class0 y data (tensor), shape=(100, 1)
x1 = torch.normal(-2*n_data, 1)  # class1 x data (tensor), shape=(100, 2)
y1 = torch.ones(100)        # class1 y data (tensor), shape=(100, 1)
x = torch.cat((x0, x1), 0).type(torch.FloatTensor) # shape (200, 2) FloatTensor = 32-bit floating
y = torch.cat((y0, y1), ).type(torch.LongTensor)  # shape (200,) LongTensor = 64-bit integer
# The code below is deprecated in Pytorch 0.4. Now, autograd directly supports tensors
# x, y = Variable(x), Variable(y)
# plt.scatter(x.data.numpy()[:, 0], x.data.numpy()[:, 1], c=y.data.numpy(), s=100, lw=0, cmap='RdYlGn')
# plt.show()

class Net(torch.nn.Module):
  def __init__(self, n_feature, n_hidden, n_output):
    super(Net, self).__init__()
    self.hidden = torch.nn.Linear(n_feature, n_hidden) # hidden layer
    self.out = torch.nn.Linear(n_hidden, n_output) # output layer
  def forward(self, x):
    x = F.relu(self.hidden(x))   # activation function for hidden layer
    x = self.out(x)
    return x
net = Net(n_feature=2, n_hidden=10, n_output=2)  # define the network
print(net) # net architecture
optimizer = torch.optim.SGD(net.parameters(), lr=0.02)
loss_func = torch.nn.CrossEntropyLoss() # the target label is NOT an one-hotted
#plt.ion() # something about plotting
for t in range(20):
  out = net(x)        # input x and predict based on x
  loss = loss_func(out, y)  # must be (1. nn output, 2. target), the target label is NOT one-hotted
  optimizer.zero_grad() # clear gradients for next train
  loss.backward()    # backpropagation, compute gradients
  optimizer.step()    # apply gradients
  if t % 2 == 0:
    # plot and show learning process
    #plt.cla()
    prediction = torch.max(out, 1)[1]
    pred_y = prediction.data.numpy()
    target_y = y.data.numpy()
    #plt.scatter(x.data.numpy()[:, 0], x.data.numpy()[:, 1], c=pred_y, s=100, lw=0, cmap='RdYlGn')
    accuracy = float((pred_y == target_y).astype(int).sum()) / float(target_y.size)
    #plt.text(1.5, -4, 'Accuracy=%.2f' % accuracy, fontdict={'size': 20, 'color': 'red'})
    print ('Accuracy=%.2f' % accuracy)
    plt.pause(0.1)
plt.show()
```

