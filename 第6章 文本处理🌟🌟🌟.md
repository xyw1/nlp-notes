# 1 文本分析流程与分词🌟🌟	

学习目标：*在本节重点探讨文本分析的经典流程以及几个经典分词算法。*

相关知识点：

*文本分析*



## PART1: 文本分词流程

就像其他的领域有自己的经典流程一样，一个文本分析的项目也有属于自己的流程。虽然每一个NLP项目有所不同，但至于流程来说没有太多本质的区别。这里会涉及到如分词、停用词过滤、文本向量的转化等步骤。在接下来两章中会对每一个做逐一的讲解。

注：顺序不固定，比如清洗可以放在分词之前，去掉无用字符，分词之后也可以做二次清洗，去掉无用单词

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210728234103124.png" alt="image-20210728234103124" style="zoom: 33%;" />

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210728234448725.png" alt="image-20210728234448725" style="zoom:33%;" />

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210728234627390.png" alt="image-20210728234627390" style="zoom:30%;" />

## PART2: 分词工具的使用



<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210728234913473.png" alt="image-20210728234913473" style="zoom:50%;" />

jieba最简单，快，轻量级，而且这个库专门使用来做分词的，设计理念简单。

SnowNLP, LTP和HanNLP也可以分词。

绝大多数情况下没有必要自己写分词工具，因为分词是一个比较通用的工具，不管是做文本摘要，还是搭建问答系统，还是做文本分类，分词本身都可以通用。除了一些特殊情况，比如这些工具没有办法满足我们的需求，可以自己写分词工具。



分词是所有工作的第一步，分词的准确性直接影响对后续任务的表现。但分词技术相对比较成熟，也有很多开源的工具可用来做中文或者对其他语言的分词。在这里，结巴分词算是最经典且简单的中文分词工具。下面以结巴分词为例来说明如何使用工具来分词。



```python
# encoding=utf-8
import jieba
# 基于jieba的分词, 结巴词库不包含"贪心学院"关键词
seg_list = jieba.cut("贪心学院专注于人工智能教育", cut_all=False)
print("Default Mode: " + "/ ".join(seg_list))
jieba.add_word("贪心学院") # 加入关键词
seg_list = jieba.cut("贪心学院专注于人工智能教育", cut_all=False)
print("Default Mode: " + "/ ".join(seg_list))
```

Out:

```txt
Building prefix dict from the default dictionary ...
Dumping model to file cache /tmp/jieba.cache
Loading model cost 1.010 seconds.
Prefix dict has been built successfully.
Default Mode: 贪心/ 学院/ 专注/ 于/ 人工智能/ 教育
Default Mode: 贪心学院/ 专注/ 于/ 人工智能/ 教育
```

### 练习

“贪心科技”,“在线”,“教育”。 但如果给定 “在线教育是”,就无法通过上述词典完整切分了。 写一个程序,输入为词典和一段文本,并判断这段文本是否能被切分成功,如果能切分返回True, 否则返回False。

```python
dic = set(["贪心科技", "人工智能", "教育", "在线", "专注于"])
def word_break(str):
    """
    完成此函数,判断str是否能被完整切分
    """
    import jieba
    for w in dic:
        jieba.add_word(w,freq=10000000) 
        jieba.suggest_freq((w),True)
    seg_list = jieba.cut(str,cut_all=False)

    for w in seg_list:
      if w not in dic:
        return False
    return True
  
assert word_break("贪心科技在线教育")==True
assert word_break("在线教育是")==False
assert word_break("")==True
assert word_break("在线教育人工智能")==True
```

(这一段代码在官网的环境报错了)

## PART3: 最大匹配算法

最大匹配算法是分词算法里面最简单也是最经典的算法，它是一种贪心算法，所以效率很高（任何算法只要使用的是贪心策略，效率都很高，但是只能得到局部最优解，不能得到全局最优解）

### 前向最大匹配 forward-max matching

1. 设置窗口大小h

2. 从第一个字符开始，截取长度为h的片段z

3. for i in 0:z-1：

   ​	if z[:h-i] in dic：

   ​			则在字符z[h-i]后分词，将分词位置记为x

   ​			break

   ​	else:

   ​			continue

   (之所以要从后往前，是为了实现最大匹配，尽量匹配最大的长度)

4. 从第x+1个字符开始，截取长度为h的片段z，重复步骤3

   <img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210731154720877.png" alt="image-20210731154720877" style="zoom: 33%;" />



参数h越大，能匹配的词的长度越长，所以h的设定也取决于词库dic里最长的长度有多长。

h越大，算法的复杂度越大，因为每次最多要循环h次才能找到结果

### 后向最大匹配算法 backward-max matching

与前向匹配算法类似，区别在于后向最大匹配算法从最后一个字符开始截取长度为h的字符串，然后从字符串前端逐步去除一个字符

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210731162047253.png" alt="image-20210731162047253" style="zoom:33%;" />

前向匹配算法和后向最大匹配算法得到的分词结果可能是不一样的（如果一样的话没有必要设计两套算法，因为他们的时间复杂度是一样的），但是在大部分情况下结果是一样的

前向匹配算法和后向最大匹配算法可以混用，名为**双向最大匹配算法**。

最大匹配算法是最简单也最经典的分词算法，但是很多开源的分词工具并不是基于最大匹配算法来进行的，它们的性能要优于最大匹配算法，虽然最大匹配算法的性能也不错，但是仍然存在一定的问题。

时间序列模型、HMM、甚至LSTM这种深度学习模型也可以用来分词。

### 匹配算法的缺点

最大匹配会选择最长的单词 “有意见” 而不是 ”有“ 和 ”意见“。也就是说它没有考虑语义。

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210731203438926.png" alt="image-20210731203438926" style="zoom:35%;" />

## PART4: 考虑语义的一种分词方法

考虑语义的方法

# 2 停用词与词的标准🌟🌟🌟

## PART1: 词的过滤

## PART2: 词的标准化

# 3 拼写纠错🌟🌟🌟

## PART1: 拼写纠错与编辑距离

## PART2: 循环词库的问题以及改进方法