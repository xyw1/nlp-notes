# 1 文本分析流程与分词🌟🌟	

学习目标：*在本节重点探讨文本分析的经典流程以及几个经典分词算法。*

相关知识点：

*文本分析*



## PART1: 文本分词流程

就像其他的领域有自己的经典流程一样，一个文本分析的项目也有属于自己的流程。虽然每一个NLP项目有所不同，但至于流程来说没有太多本质的区别。这里会涉及到如分词、停用词过滤、文本向量的转化等步骤。在接下来两章中会对每一个做逐一的讲解。

注：顺序不固定，比如清洗可以放在分词之前，去掉无用字符，分词之后也可以做二次清洗，去掉无用单词

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210728234103124.png" alt="image-20210728234103124" style="zoom: 30%;" />

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210728234448725.png" alt="image-20210728234448725" style="zoom:25%;" />

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210728234627390.png" alt="image-20210728234627390" style="zoom:25%;" />

## PART2: 分词工具的使用



<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210728234913473.png" alt="image-20210728234913473" style="zoom:30%;" />

jieba最简单，快，轻量级，而且这个库专门使用来做分词的，设计理念简单。

SnowNLP, LTP和HanNLP也可以分词。

绝大多数情况下没有必要自己写分词工具，因为分词是一个比较通用的工具，不管是做文本摘要，还是搭建问答系统，还是做文本分类，分词本身都可以通用。除了一些特殊情况，比如这些工具没有办法满足我们的需求，可以自己写分词工具。



分词是所有工作的第一步，分词的准确性直接影响对后续任务的表现。但分词技术相对比较成熟，也有很多开源的工具可用来做中文或者对其他语言的分词。在这里，结巴分词算是最经典且简单的中文分词工具。下面以结巴分词为例来说明如何使用工具来分词。



```python
# encoding=utf-8
import jieba
# 基于jieba的分词, 结巴词库不包含"贪心学院"关键词
seg_list = jieba.cut("贪心学院专注于人工智能教育", cut_all=False)
print("Default Mode: " + "/ ".join(seg_list))
jieba.add_word("贪心学院") # 加入关键词
seg_list = jieba.cut("贪心学院专注于人工智能教育", cut_all=False)
print("Default Mode: " + "/ ".join(seg_list))
```

Out:

```txt
Building prefix dict from the default dictionary ...
Dumping model to file cache /tmp/jieba.cache
Loading model cost 1.010 seconds.
Prefix dict has been built successfully.
Default Mode: 贪心/ 学院/ 专注/ 于/ 人工智能/ 教育
Default Mode: 贪心学院/ 专注/ 于/ 人工智能/ 教育
```

### 练习

“贪心科技”,“在线”,“教育”。 但如果给定 “在线教育是”,就无法通过上述词典完整切分了。 写一个程序,输入为词典和一段文本,并判断这段文本是否能被切分成功,如果能切分返回True, 否则返回False。

```python
dic = set(["贪心科技", "人工智能", "教育", "在线", "专注于"])
def word_break(str):
    """
    完成此函数,判断str是否能被完整切分
    """
    import jieba
    for w in dic:
        jieba.add_word(w,freq=10000000) 
        jieba.suggest_freq((w),True)
    seg_list = jieba.cut(str,cut_all=False)

    for w in seg_list:
      if w not in dic:
        return False
    return True
  
assert word_break("贪心科技在线教育")==True
assert word_break("在线教育是")==False
assert word_break("")==True
assert word_break("在线教育人工智能")==True
```

(这一段代码在官网的环境报错了)

## PART3: 最大匹配算法

最大匹配算法是分词算法里面最简单也是最经典的算法，它是一种贪心算法，所以效率很高（任何算法只要使用的是贪心策略，效率都很高，但是只能得到局部最优解，不能得到全局最优解）

### 前向最大匹配 forward-max matching

1. 设置窗口大小h

2. 从第一个字符开始，截取长度为h的片段z

3. for i in 0:z-1：

   ​	if z[:h-i] in dic：

   ​			则在字符z[h-i]后分词，将分词位置记为x

   ​			break

   ​	else:

   ​			continue

   (之所以要从后往前，是为了实现最大匹配，尽量匹配最大的长度)

4. 从第x+1个字符开始，截取长度为h的片段z，重复步骤3

   <img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210731154720877.png" alt="image-20210731154720877" style="zoom: 33%;" />



参数h越大，能匹配的词的长度越长，所以h的设定也取决于词库dic里最长的长度有多长。

h越大，算法的复杂度越大，因为每次最多要循环h次才能找到结果

### 后向最大匹配算法 backward-max matching

与前向匹配算法类似，区别在于后向最大匹配算法从最后一个字符开始截取长度为h的字符串，然后从字符串前端逐步去除一个字符

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210731162047253.png" alt="image-20210731162047253" style="zoom:33%;" />

前向匹配算法和后向最大匹配算法得到的分词结果可能是不一样的（如果一样的话没有必要设计两套算法，因为他们的时间复杂度是一样的），但是在大部分情况下结果是一样的

前向匹配算法和后向最大匹配算法可以混用，名为**双向最大匹配算法**。

最大匹配算法是最简单也最经典的分词算法，但是很多开源的分词工具并不是基于最大匹配算法来进行的，它们的性能要优于最大匹配算法，虽然最大匹配算法的性能也不错，但是仍然存在一定的问题。

时间序列模型、HMM、甚至LSTM这种深度学习模型也可以用来分词。

### 匹配算法的缺点

最大匹配会选择最长的单词 “有意见” 而不是 ”有“ 和 ”意见“。也就是说它没有考虑语义。

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210731203438926.png" alt="image-20210731203438926" style="zoom:35%;" />

## PART4: 考虑语义的一种分词方法

### 考虑语义的方法-1

1. 首先生成所有可能的分词

2. 使用语言模型计算每种分词的概率

3. 选择概率最大的分词形式

   

一种最简单的语言模型是unigram语言模型，它假定词与词之间是相互独立的，也就是说一个句子的概率等于各个单词的概率之乘积。

但是由于概率都是小于1的，很多小于1的项相乘在计算机当中通常会出现“**溢出**”的现象，也就是数字太小计算机无法表示的现象。

所以在实际计算当中通常把每项取log再相加。

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210801160753448.png" alt="image-20210801160753448" style="zoom:35%;" />



不难想到这种方法最大的问题在于效率问题。

### 考虑语义的方法-2 （维特比算法）

现实中可能一个文章特别长，包含了几百个单词，这时候可能的分割形式可能有几万个甚至几十万几百万个。

同时，在线上系统里面，对分词工具的时效性要求非常高。

维特比算法可以解决这样的效率问题。学计算机领域或者自然语言处理一定要知道维特比算法，它是整个计算机领域最为核心的算法之一。

在HMM或者CRF这种经典模型里面也会经常出现维特比算法。

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210801163238858.png" alt="image-20210801163238858" style="zoom:30%;" />

维特比算法的过程：

1. 把求最大概率的分词形式，转换成图论里求最短路径的问题。
2. 使用动态规划（DP）算法求最短路径
   1. 把每一个f(n)转换成f1...f(n-1)的函数
   2. 从f(1)开始计算，直到f(n)
   3. 同时使用指针记录到达f(n)的路径

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210801204328143.png" alt="image-20210801204328143" style="zoom:35%;" />

### 分词总结

- 基于匹配规则的方法
- 基于概率统计的方法
- 分词可以被认为是已经解决的问题

分词本身非常重要，任何自然语言处理领域的上层应用都取决于分词的结果，一旦分词的结果错了，后续的模块也有可能是错的。

开源的工具里面使用的分词算法都是基于时序模型的，比如HMM，CRF或者一些结合深度学习算法的模型。

关于HMM和CRF的具体细节，以及如何使用它们来做分词，将会在后续章节里面讲到。	

最后，分词本身可以被认为是已经解决的问题，即便是最大匹配算法，也可以在中文的文本里面得到非常高的准确率（95%以上），好的算法和简单算法之间差距没有那么大，使用CRF或者稍微复杂一点的算法可以得到98%或者99%的准确率，但是我们对分词的准确率和算法效率要求都很高，所以选用分词工具的时候还是要仔细挑选。

# 2 停用词与词的标准🌟🌟🌟

学习目标：*在本节，主要讲解如何过滤掉一些单词，以及如果对词做标准化。*

相关知识点：

*文本处理*

## PART1: 词的过滤

在文本处理过程中，对于有些词需要做过滤。

这些被过滤掉的单词可认为是对语义理解帮助不大，或者反而影响语义理解的单词。

同时，过滤单词有助于减小词库的大小，进而提高训练的效率和减少内存空间的使用。

### 过滤一些无用的词

无用词没有准确定义，是不是无用取决于应用场景。

无用词包括停用词、出现频率很低的词。

停用词是指经常出现在所有文档里的词，比如英文里的a,an,the,中文里也有停用词库，网上可以搜到。

出现频率低的词是指比如在语料库里出现了一次两次。

无用的词完全取决于我们对一个应用场景的判断。一个场景下的停用词可能是另一个场景下非常重要的单词。比如，我们想做一个文本主题分类，那像“好”这类形容词对我们的分类任务帮助不大，就可以作为无用词去掉；但是如果要做一个情感分类项目，对这类形容词的依赖就很高。

同时，也有可能出现一次两次的单词起到非常重要的作用。所以词频低的单词也不总是要去掉。

所以在做分词之前要去人工扫一遍语料，标记想要去掉的词。

### 去掉停用词

一些单词出现在所有的文本内，比如英文的a,an,their，这些单词可能就是没有价值的。

原因：比如我们想要做一个文本分类，我们想看的是文本之间的差异化，它们的差异在哪呢？差异就在于一部分文本使用了一部分单词，另一部分文本使用了另一些单词，所以本质上它们的差异在于它们使用了不同的单词。但是<u>停用词</u>都出现在这些文本里，所以它对文章之间的差异的贡献是很小的，所以可以把这些词去掉。

具体的中英停用词库可以在网上找到。建议在使用停用词库的时候，需要自己去扫一遍，然后做一些相应的修改，这样可能对算法更有效。要把自己认为可能有价值的词从停用词库里去掉，然后把一些新的自定义的停用词加到停用词库里面去。

### 去掉低频词

低频词也可以去掉，最主要的原因在于可以减少词库的大小，从而提高算法时间效率（自己的想法：从前面的分词算法可以看出很多步骤的时间复杂度都是随词库大小递增的）。

一个很有趣的现象：词频的分布也遵从二八定律。如下图。

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210803073429516.png" alt="image-20210803073429516" style="zoom:33%;" />

### 练习

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210803073528845.png" alt="image-20210803073528845" style="zoom:30%;" />



```python

# 方法1: 自己建立一个停用词词典
stop_words = ["the", "an", "is", "there"]
# 在使用时: 假设 word_list包含了文本里的单词
word_list = ["we", "are", "the", "students"]
filtered_words = [word for word in word_list if word not in stop_words]
print (filtered_words)
# 方法2:直接利用别人已经构建好的停用词库
from nltk.corpus import stopwords
cachedStopWords = stopwords.words("english")
# 输入你的代码
filtered_words = [word for word in word_list if word not in cachedStopWords]
print (filtered_words)
```

Out：

```txt
['we', 'are', 'students']
['students']
```



## PART2: 词的标准化

### 词的标准化: Stemming

英文的单词有很多种不同的形式，它们都属于同一个单词

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210803074127483.png" alt="image-20210803074127483" style="zoom:30%;" />

词的标准化有两种方式，一种是Stemming,一种是Lemmazation

Stemming的过程：

把单词转换成词根（未必在词库里），但是后续生成的是向量，所以不影响操作。

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210803075541686.png" alt="image-20210803075541686" style="zoom:30%;" />

Lemmazation比Stemming的规则更强硬一点，通过Lemmazation生成的单词一定在词库里，因为这个算法把词库当成一个很重要的原料，所以它要保证的是合并出来的单词一定是存在于词库里的，所以用Lemmazation再把这些单词（是说的单词还是Stemming合并出来的词根？）再做一个Normalization，你会发现每个单词是比较合理的单词（没听懂这句话含义）

### Porter Stemmer

Stemming的一个经典算法叫Porter Stemmer，网上可以查到它的源代码。

Porter Stemmer算法是由一系列if...else...构成

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210803080628503.png" alt="image-20210803080628503" style="zoom:30%;" />

比如，Step 1a中，把sess替换成ss等

Step 1b中，把现在进行时和过去式替换成现在时，

.......

这一系列替换是根据语言学家的专家知识得到的

### 代码示例

```python

from nltk.stem.porter import *
stemmer = PorterStemmer()
test_strs = ['caresses', 'flies', 'dies', 'mules', 'denied',
    'died', 'agreed', 'owned', 'humbled', 'sized',
    'meeting', 'stating', 'siezing', 'itemization',
    'sensational', 'traditional', 'reference', 'colonizer',
    'plotted']
singles = [stemmer.stem(word) for word in test_strs]
print(' '.join(singles)) # doctest: +NORMALIZE_WHITESPACE
```

Out:

```txt
caress fli die mule deni die agre own humbl size meet state siez item sensat tradit refer colon plot
```



# 3 拼写纠错🌟🌟🌟

## PART1: 拼写纠错与编辑距离

## PART2: 循环词库的问题以及改进方法