# 1 词向量基础🌟🌟

学习目标：*本节主要介绍关于词向量的一些基础概念，为后续的模型讲解做铺垫。*

相关知识点：

*词向量*

## PART1: 单词的表示

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210809073112203.png" alt="image-20210809073112203" style="zoom:30%;" />

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210809073147963.png" alt="image-20210809073147963" style="zoom:30%;" />

## PART2: 从独热编码到分布式表示

## PART3: 词向量的训练

词向量是训练出来的。所以，我们可以认为，中间有一个模型可以帮助我们训练出每个单词的向量。这个模型到底是什么呢?这个黑盒子就是我们即将要解开的秘密。词向量技术也是带动NLP发展的最有利的催化器，自从2013年提出word2vec开始，之后整个NLP领域有了飞跃式的发展。如果说，ImageNet是CV领域的催化剂，那么word2vec有着同样的重要性。

### 通过模型学习词向量

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210809074044954.png" alt="image-20210809074044954" style="zoom:30%;" />

那到底这个黑盒子里是什么呢?在具体剖析词向量模型之前，我们先看一下如何用一些模型来训练出词向量。也就是给定一个语料库，它的输出长得是怎样的。 下面看一段词向量训练过程的视频。在视频中所使用的代码链接在gitlab上。

### 代码讲解

网上摘取文本当作语料库，用word2vec.py脚本训练，生成词向量。代码在gitlab有。

### 总结

好了，到目前为止我们简单地了解了词向量以及它的训练是怎样的。在这里做个简单的总结:

- 词向量可以认为在某种程度上代表单词的含义
- 词向量是需要训练出来的，也就是提前要设计好词向量训练模型
- 词向量技术极大推动了NLP领域的发展

# 2 SkipGram模型详解🌟🌟🌟🌟

学习目标：*在本节，我们重点来学习SkipGram模型的细节，这是最为经典的词向量训练模型，有必要掌握。*

相关知识点：

*SkipGram模型*

## PART1: 训练词向量的核心思想

词向量模型其实很多，包括大家所熟悉的BERT等。每一种词向量，它的目标和作用是不一样的，我们在后续的章节中会一一做介绍。但不管怎么样，这些模型都享有着共同的核心思想。等我们深入理解了这个思想，便可以更容易理解模型为什么会这么设计，而且甚至将来也可以提出自己的模型。

首先说明一点，**<u>词向量的学习通常是无监督学习</u>**，也就是不需要标注好的文本。那对于这样的无监督学习，我们应该如何合理地设计目标函数并学出词向量呢?

### 学习词向量 - 核心思想

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210809080306296.png" alt="image-20210809080306296" style="zoom:30%;" />

一个非常重要的假设：分布式假设。就是可以通过一个单词周围的单词知道这个单词是什么。

分布式假设是后面几乎所有词向量的核心。

可以根据词向量模型根据上下文预测每个单词是否正确来建立损失函数。

### 练习

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210809082600917.png" alt="image-20210809082600917" style="zoom:30%;" />

## PART2: SkipGram的目标函数

## PART3: SkipGram的负采样

# 3 其他词向量技术🌟🌟

## PART1: 矩阵分解法

## PART2: Glove向量

## PART3: 高斯词嵌入

## PART4: 词向量总结

# 4 论文解读：基于SkipGram的Airbnb房屋推荐🌟🌟🌟

## PART1: 论文解读