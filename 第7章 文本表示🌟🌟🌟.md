

# 1 文本表示基础🌟

学习目标：*本节重点介绍如何用向量来表示一个单词或者文本，这是文本表示的基础，也是最重要的内容*

相关知识点：

*文本表示*

## PART1: 单词的表示 done

对于自然语言处理各类应用，最基础的任务为文本表示。因为我们都知道一个文本是不能直接作为模型的输入的，所以我们必须要先把文本转换成向量的形式之后，再导入到模型中训练。所谓文本的表示，其实就是研究如何把文本表示成向量或者矩阵的形式。

文本的最小单元为单词，其次为短语、句子、或者段落。我们需要懂得如何把这些表示成向量的形式。其中，单词的表示法是最基础的。另外，对于句子或者更长的文本来说，它们的表示依赖于单词的表示法。 在这里想说的一点是，单词的表示法不止一种，比如有独热编码的表示法，词向量的表示法等等。

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210804082541194.png" alt="image-20210804082541194" style="zoom:30%;" />

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210801220738434.png" alt="image-20210801220738434" style="zoom:33%;" />

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210801220748417.png" alt="image-20210801220748417" style="zoom:33%;" />

## PART2: 句子的表示 

知道了如何表示一个单词之后，我们很自然地就可以得到如何表示一个句子了。一个句子由多个单词来组成，那实际上记录一下哪些单词出现，哪些单词没有出现就可以了。当然，很多时候我们也需要记录一个单词所出现的次数。

### 句子的表示（0/1表示）done

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210801214914336.png" alt="image-20210801214914336" style="zoom:33%;" />

也可称作Boolean Vector，只记录一个单词有没有出现，没有记录出现了多少次

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210801214807862.png" alt="image-20210801214807862" style="zoom:30%;" />

### 句子的表示（单词个数）done

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210801215449319.png" alt="image-20210801215449319" style="zoom:30%;" />

这种表示法也叫做Count Vector

代码示例：

```python
from sklearn.feature_extraction.text import CountVectorizer
corpus = [
  'I like this course.',
  'I like this game.',
  'I like this course, but I also like that game',
]
# 构建countvectorizer object
vectorizer = CountVectorizer()
# 得到每个文档的count向量
X = vectorizer.fit_transform(corpus)
# 打印词典
print(vectorizer.get_feature_names())
# 打印每个文档的向量
print(X.toarray())
```

Out:

```txt
['also', 'but', 'course', 'game', 'like', 'that', 'this']
[[0 0 1 0 1 0 1]
 [0 0 0 1 1 0 1]
 [1 1 1 1 2 1 1]]
```

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210804194632092.png" alt="image-20210804194632092" style="zoom:30%;" />

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210804194744087.png" alt="image-20210804194744087" style="zoom:30%;" />

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210804194842401.png" alt="image-20210804194842401" style="zoom:33%;" />

## PART3: tf-idf向量

### 句子的表示

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210804195247403.png" alt="image-20210804195247403" style="zoom:33%;" />

所以，如果只记录单词的个数也是不够的，我们还需要考虑单词的权重，也可以认为是质量。这有点类似于，一个人有很多朋友不代表这个人有多厉害，还需要社交的质量，其实是同一个道理。 那如何把这种所谓的“质量”引入到表示中呢?答案是tf-idf

### Tf-idf表示

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210804195639488.png" alt="image-20210804195639488" style="zoom:33%;" />

单单是tf(d,w)就是count vector,加上idf(w)就是tf-idf

当N(w)大时，说明该词语出现在很多的文档里，idf权重就小。权重要加log是因为不想让差异太大

tf-idf的应用非常广泛，即便放在当前，也是表示文本的最核心的技术之一。 之前我们讲过什么是基准，那<u>tf-idf是文本表示领域的最有效的基准</u>。很多时候，<u>基于深度学习的文本表示也未必要优于tf-idf的表示。</u>

### Tf-idf计算过程

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210804201040847.png" alt="image-20210804201040847" style="zoom:33%;" />

早期的文本系统基本都用的是Tf-idf表示，sklearn里面已经集成了Tf-idf的表示，只需要提供语料库并调用模块就可以实现。

# 2文本相似度🌟🌟🌟

学习目标：*在本节，我们来学习一下如何计算两个文本之间的相似度。*

相关知识点：

*文本相似度*

## PART1: 计算欧式距离

如何计算两个文本之间的相似度?这个问题实际上可以认为是计算两个向量之间的相似度。因为通过上一节的内容已经知道了如何把文本转换成向量。 所以本节所涉及到的相似度计算公式适合任何向量化的场景，不仅仅局限于文本之间的相似度。有两种常见的相似度计算方法，分别为基于欧式距离的计算，另外一种方式为基于余弦相似度的计算。

### 句子相似度计算-欧氏距离

单词表示和句子表示是NLP领域的基础，但是我们最终的目的是想了解文本的语义。相理解文本语义的一种方式是，如果你知道一个句子的语义，那么计算另一个句子和它的相似度，如果相似度高，那么这两个句子具有相似的语义。所以基于相似度计算的方式是一种很重要的理解语义的方式。

而且在很多NLP应用场景中也需要去做类似的事情，比如一个问答系统，用户问了一句话，然后去匹配问答库里的其他问题，然后把跟这个问题相似度最高的问题的答案返回。

欧氏距离就是数学里的欧氏距离：

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210804211334610.png" alt="image-20210804211334610" style="zoom:50%;" />



### 练习

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210804211634071.png" alt="image-20210804211634071" style="zoom:33%;" />

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210804211924060.png" alt="image-20210804211924060" style="zoom:33%;" />

## PART2: 计算余弦相似度

为了弥补欧式距离所存在的问题，我们不得不要提出另外一种相似度计算方法，这就是最著名的方法-余弦相似度。通过余弦相似度事实上我们计算的是两个向量之间的夹角大小。两个向量的方向上越一致就说明它俩的相似度就越高。

### 句子相似度计算-余弦相似度

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210805074634176.png" alt="image-20210805074634176" style="zoom:25%;" />

点积是按项相乘再求和

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210805072720968.png" alt="image-20210805072720968" style="zoom:33%;" />

（自己的思考：如果余弦相似度计算的是x和y之间的夹角，那欧氏距离计算的就是x和y断点的几何距离。

### 练习

给定两个向量,请计算两个向量之间的余弦相似度

```python
import numpy as np
def cos_sim(a, b):
	"""给定两个向量,a和b,计算它俩之间的余弦相似度
	"""
    # TODO:
	dot_product = np.dot(a,b)
	norm_a = np.sqrt((a**2).sum())
	norm_b = np.sqrt((b**2).sum())
	return dot_product / (norm_a * norm_b)
# TEST CASE
sentence_m = np.array([1, 1, 1, 1, 0, 0, 0, 0, 0])
sentence_h = np.array([0, 0, 1, 1, 1, 1, 0, 0, 0])
sentence_w = np.array([0, 0, 0, 1, 0, 0, 1, 1, 1])
print(cos_sim(sentence_m, sentence_h)) # 0.5
print(cos_sim(sentence_m, sentence_w)) # 0.25
```

Out:

```txt
0.5
0.25
```





<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210805075124210.png" alt="image-20210805075124210" style="zoom:33%;" />

注意选项B和D。

# 3 词向量基础🌟🌟

学习目标：*本节重点介绍词向量的核心思想，以及它的应用。*

相关知识点：

*拼写纠错*

## PART1: 计算单词之间的相似度

上一节为止，我们一直在讨论如何计算两个文本之间的相似度，但至今还没有讨论过如何计算<u>两个单词之间的相似度</u>。单词作为文本的最基本的要素，如何表示单词的含义以及两个单词之间的相似度也极其重要。我们一起来了解一下在<u>独热编码的基础下，如何计算两个单词之间的相似度</u>。

### 单词之间相似度计算

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210805075829362.png" alt="image-20210805075829362" style="zoom:33%;" />

One-hot编码的单词，用欧氏距离计算的相似度都是根号2，余弦相似度都是0。

在这个场景下我们的需求是，比如给定CPU，GPU和自然语言处理三个词汇，CPU和GPU之间的相似度要大于它们和自然语言处理的相似度。

那么问题要么出现在表示方法上，要么出现在相似度计算上。

（自己的思考：词向量。）



显然是，通过欧式距离或者余弦相似度是没有办法算出单词之间的相似度，因为不管我们怎么计算，俩俩之间的结果都是一样的。那问题到底处在哪儿呢?答案是，一开始的独热编码的表示!既然独热编码表示不支持计算两个单词之间的相似度，我们需要想另外一种单词的表示法了，这就自然引出词向量的概念。 除了不能计算相似度，独热编码也存在稀疏性的问题。

### 另一个问题-稀疏性（Sparsity）

独热编码的结果大部分都是0

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210805080752848.png" alt="image-20210805080752848" style="zoom:25%;" />

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210805080843691.png" alt="image-20210805080843691" style="zoom:25%;" />

## PART2: 词向量基础

### 从独热编码到词向量

独热编码是一个表示系统，之所以称它为表示系统，是因为一个单词可以用独热编码表示，在这个基础之上可以把句子表示成count vector或者tf-idf，所以它是贯穿在一个系统上的。但是这样一个系统有问题，第一个问题在于，在这样一个系统下没有办法表示单词之间的相似度，另一个小问题是独热编码的稀疏性。

为了解决这两个问题，提出一种词向量表示方法，词向量表示法，这是一种分布式表示法。

每个单词独热编码的长度就是词库的大小，而词向量表示法的长度和词库大小没有任何关系，这个维度是提前指定好的，比如100维，200维。这种参数也叫**超参数**。词向量表示法的大小远小于独热编码的大小，词库大小可能有几十万，而词向量只需要用100维300维的向量。

词向量是训练出来的。

独热编码和词向量是两种不同的表示路线，一旦走了词向量的路线，句子的表示可能会使用count vector或者tf-idf。如果使用了词向量表示方法，后面的句子也是沿着词向量这一条路线来表示。有时候也可以把这两种方法结合在一起使用。

后面的章节会讲到词向量是如何训练出来的。

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210806074934896.png" alt="image-20210806074934896" style="zoom:30%;" />

### 基于词向量的相似度比较

视频里根据欧氏距离计算了词向量之间的相似度。我们训练的词向量越优秀，对于相似度应该很高的词汇，计算出来的相似度也就越高。

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210806074946991.png" alt="image-20210806074946991" style="zoom:30%;" />

我们可以看到在分布式表示方法下，两个单词之间的相似度是可以算出来的。当然，效果取决于词向量的质量。所以，接下来的话题是如何得出这些词向量?在这一章节，我们只做简单的介绍，具体详细的方法论贯穿之后的很多的章节中。

### 学习词向量

抽象的理解就是，给定语料库，通过一个黑盒子，生成词向量。

这个黑盒子可能是像bert, ELMo, Glove, SkipGram等，这些方法论实际都是为了得出词向量的。模型之间有差别，可能一些模型训练出来的词向量更适合一定的场景。

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210806080328030.png" alt="image-20210806080328030" style="zoom:30%;" />

### 词向量的含义

我们设计词向量的最终目的是为了表示单词的含义。含义本身比较抽象，人能理解一个句子的含义，但是这些含义能不能通过数字化的方式表示呢？含义怎么去量化，这是多年来NLP社区和研究者一直子在考虑的一个问题。目前的主流技术就是通过词向量来表示单词或者句子的含义。但是如果未来有更好的表示含义的方式，词向量也会被替代。

词向量是一个向量，有方向，可以在空间当中画出来，一个好的词向量，可以让相似的词汇在空间中距离很近。

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210806081322632.png" alt="image-20210806081322632" style="zoom:33%;" />

### 练习

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210806081408335.png" alt="image-20210806081408335" style="zoom:30%;" />

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210806081423178.png" alt="image-20210806081423178" style="zoom:25%;" />

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210806081435453.png" alt="image-20210806081435453" style="zoom:30%;" />

以上图片中是否看到了一些有趣的现象?语义上比较相似的单词聚集在了一起，这其实变相地说明，词向量在某种意义上表达出了一个单词的含义。为了可视化词向量而使用的降维技术通常包括 https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html )，也是一种常用的降维算法。

## PART3: 句子向量

“假如我们手里已经有了训练好的词向量，那如何通过这些词向量来表示一个完整的文本呢，或者一个句子呢?有一种最简单且常用的方法，就是做平均!”

### 从词向量到句子向量

到目前为止我们讨论的都是单词的向量，那么如何去表示一个句子呢？一种简单的方法就是做平均。



<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210806081744570.png" alt="image-20210806081744570" style="zoom:30%;" />

所以做平均是一个简单并且还算有效的一种方法，但是这个方法未必是一个很好的方法，比如有些单词没那么重要，在做平均的时候也会考虑进去。

句子向量的质量对单词向量的质量依赖度很高。

在后面的章节会讨论如何通过其他的方式表示句子。

### 小结

有了文本表示之后，我们就可以开始对文本做建模了，比如计算两个文本之间的相似度，或者对某个文本做分类。在这里我们来做个简单的小结:

- 单词的独热编码和分布式表示是两种完全不一样的编码方式
- 这两种不同的编码方式是目前文本表示的两个方向，有些时候传统的独热编码的方式可能更适合，有些时候分布式表示法更适合，具体还是要通过测试来获得结论
- 独热编码的最大的问题是不能表示一个单词的含义
- 词向量的质量取决于词向量训练模型，不同的模型所给出的结果是不一样的