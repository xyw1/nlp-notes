

# 1 文本表示基础🌟

学习目标：*本节重点介绍如何用向量来表示一个单词或者文本，这是文本表示的基础，也是最重要的内容*

相关知识点：

*文本表示*

## PART1: 单词的表示 done

对于自然语言处理各类应用，最基础的任务为文本表示。因为我们都知道一个文本是不能直接作为模型的输入的，所以我们必须要先把文本转换成向量的形式之后，再导入到模型中训练。所谓文本的表示，其实就是研究如何把文本表示成向量或者矩阵的形式。

文本的最小单元为单词，其次为短语、句子、或者段落。我们需要懂得如何把这些表示成向量的形式。其中，单词的表示法是最基础的。另外，对于句子或者更长的文本来说，它们的表示依赖于单词的表示法。 在这里想说的一点是，单词的表示法不止一种，比如有独热编码的表示法，词向量的表示法等等。

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210804082541194.png" alt="image-20210804082541194" style="zoom:30%;" />

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210801220738434.png" alt="image-20210801220738434" style="zoom:33%;" />

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210801220748417.png" alt="image-20210801220748417" style="zoom:33%;" />

## PART2: 句子的表示 

知道了如何表示一个单词之后，我们很自然地就可以得到如何表示一个句子了。一个句子由多个单词来组成，那实际上记录一下哪些单词出现，哪些单词没有出现就可以了。当然，很多时候我们也需要记录一个单词所出现的次数。

### 句子的表示（0/1表示）done

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210801214914336.png" alt="image-20210801214914336" style="zoom:33%;" />

也可称作Boolean Vector，只记录一个单词有没有出现，没有记录出现了多少次

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210801214807862.png" alt="image-20210801214807862" style="zoom:30%;" />

### 句子的表示（单词个数）done

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210801215449319.png" alt="image-20210801215449319" style="zoom:30%;" />

这种表示法也叫做Count Vector

代码示例：

```python
from sklearn.feature_extraction.text import CountVectorizer
corpus = [
  'I like this course.',
  'I like this game.',
  'I like this course, but I also like that game',
]
# 构建countvectorizer object
vectorizer = CountVectorizer()
# 得到每个文档的count向量
X = vectorizer.fit_transform(corpus)
# 打印词典
print(vectorizer.get_feature_names())
# 打印每个文档的向量
print(X.toarray())
```

Out:

```txt
['also', 'but', 'course', 'game', 'like', 'that', 'this']
[[0 0 1 0 1 0 1]
 [0 0 0 1 1 0 1]
 [1 1 1 1 2 1 1]]
```

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210804194632092.png" alt="image-20210804194632092" style="zoom:30%;" />

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210804194744087.png" alt="image-20210804194744087" style="zoom:30%;" />

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210804194842401.png" alt="image-20210804194842401" style="zoom:33%;" />

## PART3: tf-idf向量

### 句子的表示

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210804195247403.png" alt="image-20210804195247403" style="zoom:33%;" />

所以，如果只记录单词的个数也是不够的，我们还需要考虑单词的权重，也可以认为是质量。这有点类似于，一个人有很多朋友不代表这个人有多厉害，还需要社交的质量，其实是同一个道理。 那如何把这种所谓的“质量”引入到表示中呢?答案是tf-idf

### Tf-idf表示

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210804195639488.png" alt="image-20210804195639488" style="zoom:33%;" />

单单是tf(d,w)就是count vector,加上idf(w)就是tf-idf

当N(w)大时，说明该词语出现在很多的文档里，idf权重就小。权重要加log是因为不想让差异太大

tf-idf的应用非常广泛，即便放在当前，也是表示文本的最核心的技术之一。 之前我们讲过什么是基准，那<u>tf-idf是文本表示领域的最有效的基准</u>。很多时候，<u>基于深度学习的文本表示也未必要优于tf-idf的表示。</u>

### Tf-idf计算过程

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210804201040847.png" alt="image-20210804201040847" style="zoom:33%;" />

早期的文本系统基本都用的是Tf-idf表示，sklearn里面已经集成了Tf-idf的表示，只需要提供语料库并调用模块就可以实现。

# 2文本相似度🌟🌟🌟

学习目标：*在本节，我们来学习一下如何计算两个文本之间的相似度。*

相关知识点：

*文本相似度*

## PART1: 计算欧式距离

如何计算两个文本之间的相似度?这个问题实际上可以认为是计算两个向量之间的相似度。因为通过上一节的内容已经知道了如何把文本转换成向量。 所以本节所涉及到的相似度计算公式适合任何向量化的场景，不仅仅局限于文本之间的相似度。有两种常见的相似度计算方法，分别为基于欧式距离的计算，另外一种方式为基于余弦相似度的计算。

### 句子相似度计算-欧氏距离

单词表示和句子表示是NLP领域的基础，但是我们最终的目的是想了解文本的语义。相理解文本语义的一种方式是，如果你知道一个句子的语义，那么计算另一个句子和它的相似度，如果相似度高，那么这两个句子具有相似的语义。所以基于相似度计算的方式是一种很重要的理解语义的方式。

而且在很多NLP应用场景中也需要去做类似的事情，比如一个问答系统，用户问了一句话，然后去匹配问答库里的其他问题，然后把跟这个问题相似度最高的问题的答案返回。

欧氏距离就是数学里的欧氏距离：

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210804211334610.png" alt="image-20210804211334610" style="zoom:50%;" />



### 练习

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210804211634071.png" alt="image-20210804211634071" style="zoom:33%;" />

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210804211924060.png" alt="image-20210804211924060" style="zoom:33%;" />

## PART2: 计算余弦相似度

为了弥补欧式距离所存在的问题，我们不得不要提出另外一种相似度计算方法，这就是最著名的方法-余弦相似度。通过余弦相似度事实上我们计算的是两个向量之间的夹角大小。两个向量的方向上越一致就说明它俩的相似度就越高。

### 句子相似度计算-余弦相似度

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210805074634176.png" alt="image-20210805074634176" style="zoom:25%;" />

点积是按项相乘再求和

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210805072720968.png" alt="image-20210805072720968" style="zoom:33%;" />

（自己的思考：如果余弦相似度计算的是x和y之间的夹角，那欧氏距离计算的就是x和y断点的几何距离。

### 练习

给定两个向量,请计算两个向量之间的余弦相似度

```python
import numpy as np
def cos_sim(a, b):
	"""给定两个向量,a和b,计算它俩之间的余弦相似度
	"""
    # TODO:
	dot_product = np.dot(a,b)
	norm_a = np.sqrt((a**2).sum())
	norm_b = np.sqrt((b**2).sum())
	return dot_product / (norm_a * norm_b)
# TEST CASE
sentence_m = np.array([1, 1, 1, 1, 0, 0, 0, 0, 0])
sentence_h = np.array([0, 0, 1, 1, 1, 1, 0, 0, 0])
sentence_w = np.array([0, 0, 0, 1, 0, 0, 1, 1, 1])
print(cos_sim(sentence_m, sentence_h)) # 0.5
print(cos_sim(sentence_m, sentence_w)) # 0.25
```

Out:

```txt
0.5
0.25
```





<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210805075124210.png" alt="image-20210805075124210" style="zoom:33%;" />

注意选项B和D。

# 3 词向量基础🌟🌟

学习目标：*本节重点介绍词向量的核心思想，以及它的应用。*

相关知识点：

*拼写纠错*

## PART1: 计算单词之间的相似度

上一节为止，我们一直在讨论如何计算两个文本之间的相似度，但至今还没有讨论过如何计算<u>两个单词之间的相似度</u>。单词作为文本的最基本的要素，如何表示单词的含义以及两个单词之间的相似度也极其重要。我们一起来了解一下在<u>独热编码的基础下，如何计算两个单词之间的相似度</u>。

### 单词之间相似度计算

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210805075829362.png" alt="image-20210805075829362" style="zoom:33%;" />

One-hot编码的单词，用欧氏距离计算的相似度都是根号2，余弦相似度都是0。

在这个场景下我们的需求是，比如给定CPU，GPU和自然语言处理三个词汇，CPU和GPU之间的相似度要大于它们和自然语言处理的相似度。

那么问题要么出现在表示方法上，要么出现在相似度计算上。

（自己的思考：词向量。）



显然是，通过欧式距离或者余弦相似度是没有办法算出单词之间的相似度，因为不管我们怎么计算，俩俩之间的结果都是一样的。那问题到底处在哪儿呢?答案是，一开始的独热编码的表示!既然独热编码表示不支持计算两个单词之间的相似度，我们需要想另外一种单词的表示法了，这就自然引出词向量的概念。 除了不能计算相似度，独热编码也存在稀疏性的问题。

### 另一个问题-稀疏性（Sparsity）

独热编码的结果大部分都是0

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210805080752848.png" alt="image-20210805080752848" style="zoom:25%;" />

<img src="/Users/yunwanxu/Library/Application Support/typora-user-images/image-20210805080843691.png" alt="image-20210805080843691" style="zoom:25%;" />

## PART2: 词向量基础

## PART3: 句子向量